# iwinv Ollama / vLLM GPU 서버 전략 메모

**작성일**: 2025-11-19  
**맥락**: 온프레미스 NixOS 클러스터(RTX 5080 ×3 + storage) + iwinv 클라우드 GPU 리소스를 혼합 활용. 약 900만 원 크레딧을 바탕으로 Ollama / vLLM 상품을 어떻게 조합해 쓸지 정리.

---

## 1. iwinv Ollama / vLLM 상품 개요

### 1.1 Ollama 상품 요약

- **포지션**: "CPU + 1 GPU" 혼합에 특화된 **가성비 LLM 서버**
  - 4bit/8bit 양자화 LLM을 **적은 VRAM + CPU 메모리** 조합에서 돌리는 데 최적화.
  - Ollama + KTransformers 환경을 전제로 한 VM/베어메탈 구성 제공.
- **활용 영역 (iwinv 정의)**
  - 지능형 챗봇 플랫폼 및 생성형 API 시스템
  - RAG 및 검색 기반 AI 서비스
  - 블록체인 + AI 연계 DApp
  - UX/비주얼 데이터 최적화 서비스 등
- **대표 하드웨어 옵션 예시**
  - AMD Radeon Pro W6800 베어메탈 (32GB VRAM)
  - AMD RX 9060XT, AMD AI GPU K6 등 **AMD 계열 Ollama 특화 서버**
  - NVIDIA Tesla T4, RTX 4000 Ada, RTX A6000 등 VM에서도 Ollama 지원
- **성능/비용 예시 (일부)**
  - Gemma3 27B (Q4_K_M, 128K 컨텍스트)
    - W6800 32GB ×1, 16vCPU / 64GB / 1TB SSD, Ollama 0.9.0 → 약 16.85 tok/s, 월 199,000원 수준
  - DeepSeek-R1 671B (KTransformers)
    - 대형 Xeon + 1TB DRAM 구성으로, tok/s 7~9 수준 (견적 문의형 상품)
- **모델 가이드**
  - gpt-oss, Qwen3, DeepSeek-R1, Gemma3, Llama4 Scout, Llama3.x, Phi4, HyperCLOVA X 등 **주요 오픈소스 LLM을 Ollama 환경에서 돌리는 전제를 공식적으로 안내**.
  - Q4 양자화 기준 **모델 크기 vs 최소 DRAM/VRAM** 가이드 제공:
    - ~2B: 4–8GB DRAM (노트북/CPU-only도 가능)
    - 2–10B: 8–16GB (소비자용 GPU)
    - 10–20B: 16–32GB (4000Ada 이상, 4090/5090 Multi-GPU)
    - 20–70B: 32–128GB (A6000, PRO5000 Multi-GPU)
    - 70B~: 128GB+ (PRO6000 Multi-GPU 등)

### 1.2 vLLM 상품 요약

- **포지션**: Multi-GPU 기반 **고성능 LLM 추론/서빙 프레임워크**용 서버
  - PagedAttention, 비동기 요청 처리, 배치 최적화, FP16/BF16 CUDA 최적화 등 vLLM 기능에 맞춘 환경 제공.
- **핵심 특징 (iwinv 설명 기준)**
  - 메모리 효율성: PagedAttention으로 긴 컨텍스트를 VRAM 절약하면서 처리.
  - 비동기/배치 처리: 다수 요청을 GPU 배치로 묶어 고효율 서빙.
  - 확장성: 단일/다중 GPU, VM/베어메탈/맞춤형 서버까지 다양한 형태 지원.
  - 활용 영역: 고성능 챗봇, 문서 요약, RAG, 엔터프라이즈 AI 백엔드, LLM 기반 SaaS 등.
- **성능/비용 예시**
  - vLLM 0.9.1 기준 벤치마크 (가상 서버)
    - RTX PRO 6000 ×1 (8vCPU / 60GB / 100GB SSD)
      - GPT-OSS 120B: ~100 tok/s, DRAM 63GB, VRAM 63GB, 월 1,390,000원
      - Gemma3 27B: ~49 tok/s, DRAM 17.6GB, VRAM 19GB, 월 1,390,000원
    - RTX PRO 6000 ×2 (16vCPU / 120GB / 100GB SSD)
      - Qwen3 235B: ~54 tok/s, DRAM 134GB, VRAM 68GB, 월 2,780,000원
- **지원 GPU 옵션**
  - VM: Tesla T4, RTX 4000 Ada, A6000, PRO5000, PRO6000 등 (Ollama & vLLM 동시 지원)
  - 맞춤형 서버: 최대 4 or 8 GPU 구성, PRO6000/PRO5000/4090 blower 등 선택, 10Gbps NIC 옵션.

---

## 2. 우리의 전략적 관점: 온프레미스 + iwinv 혼합

### 2.1 온프레미스(hej-nixos-cluster + nixos-config)의 역할

- **역할**: "사내 지식베이스 + 임베딩·리랭킹 + RAG 파이프라인"을 24/7로 돌리는 **지식 정련 공장**.
  - RTX 5080 ×3 (온프레미스): 내부 문서/로그/노트/티켓을 계속 갈아넣고 재구성.
  - NFS + 10G 네트워크 + 스토리지 노드는 온프레미스에 맞춤 최적화.
- **장점**
  - 토큰 비용 없이, 내부 데이터를 마음껏 재가공/재임베딩.
  - 임베딩/리랭킹/인덱싱/피드백 루프 전체를 **NixOS + Git + 에이전트**로 통제.

### 2.2 iwinv 클라우드의 역할

- **역할**: 특정 상황에서 필요한 **추가 GPU 파워 + 다양한 GPU 아키텍처 실험 플랫폼**.
  - 예: PRO 6000 Multi-GPU로 **대형 LLM 벤치마크 및 비교 실험**.
  - 예: AMD W6800 베어메탈로 **Ollama 전용 저비용 Always-on 노드**.
- **우리가 얻을 수 있는 것**
  - 온프레미스와 다른 GPU (PRO6000, A6000, W6800 등) 위에서 **동일 LLM + 동일 프롬프트**로 성능/비용 곡선을 측정.
  - 900만 원 크레딧으로, 2~3개 시나리오를 병렬로 장기간 실험하고 최적 조합을 찾을 수 있음.

---

## 3. Ollama 상품을 활용하는 전략

### 3.1 활용 목적

- **CPU+1GPU 기반의 값싼 LLM/챗봇 실험 및 라이트워크로드 운영**
  - 사내용 간단한 챗봇, PoC, 조용한 시간대에 돌리는 잡.
  - 고성능이 아니라 **항상 켜두기 쉬운 서버**가 목표.
- **대상 모델**
  - Gemma3 12B/27B, Qwen3 30B, DeepSeek-R1 7B/14B 등 **중형 이하 모델**.
  - HyperCLOVA X SEED 1.5B/3B 같은 한국어 특화 경량 모델.

### 3.2 추천 패턴(예시)

1. **AMD W6800 베어메탈 1대 + Ollama**
   - 장점: VRAM 32GB로 8B~27B급 모델을 안정적으로 구동.
   - 월 20만 원 안팎의 비용으로 **Low-latency PoC/내부툴용 LLM** 운영 가능.
   - iwinv가 공식적으로 Ollama 지원 GPU로 테스트한 구성이므로 운영 리스크가 낮음.

2. **Tesla T4 VM 1대 (T4.G1/G2)**
   - Ollama & vLLM 겸용 환경에서, 주로 Ollama 위주의 라이트 워크로드.
   - 토큰 속도는 PRO6000 대비 낮지만 비용도 낮음.

이 서버들은 **온프레미스와 같은 네트워크에 직접 붙이지 않아도** 되고, 
HTTP+VPN+프록시 레이어를 통해 특정 워크로드(코드 도우미, PoC, 카피라이팅 등)를 외부 LLM과 혼합해서 처리하도록 설계할 수 있다.

---

## 4. vLLM 상품을 활용하는 전략

### 4.1 활용 목적

- **고성능 LLM 추론 + Multi-GPU 실험 플랫폼**
  - 대형 모델 (70B 이상) 또는 초고속 추론이 필요한 실험을 온프레미스 대신 클라우드에서 수행.
  - GPU 종류/갯수/DRAM 용량에 따른 **성능-비용 프로파일링**을 수집.
- **우리의 주요 시나리오**
  - GPT-OSS 20B/120B, Qwen3 235B, Llama3.3 70B, DeepSeek-R1 70B 등을 vLLM 기반으로 돌려보고,
  - 온프레미스 RTX 5080 ×3와 비교하여 **“사내 KB에 적합한 핵심 LLM 조합”** 을 찾는 것.

### 4.2 추천 패턴(예시)

1. **RTX PRO 6000 VM 1대 (PRO6000.G1)**
   - 1× PRO 6000, 8 vCPU, 216GB 메모리, 96GB VRAM.
   - vLLM 0.9.1 기준 GPT-OSS 120B에서 100 tok/s 레벨의 벤치마크.
   - 월 149만 원 선으로 **고성능 단일 노드 LLM 실험**에 적합.

2. **RTX PRO 6000 VM 2대 (또는 2GPU VM)**
   - Qwen3 235B 등 초대형 모델을 대상으로 **멀티-GPU 병렬 처리 실험**.
   - 월 2.7~3백만 원 수준에서 짧은 기간 집중 실험용으로 사용.

3. **맞춤형 GPU 서버 (견적형)**
   - PRO6000 ×4/8, PRO5000, 4090 blower 등 원하는 GPU 구성으로 전용 박스를 세워,
   - 장기간 RAG+vLLM 벤치마크, 서빙 실험을 수행.
   - 900만 원 크레딧으로 1~3개월 집중 실험 프로젝트 구성이 가능.

---

## 5. 900만 원 크레딧을 활용한 제안 시나리오

### 시나리오 A: "Ollama 상시 + vLLM 단기 실험"

- **구성**
  - Ollama용 W6800 베어메탈 1대: 월 ~20만 × 6개월 = 120만 원
  - PRO6000.G1 vLLM VM: 월 ~149만 × 3개월 = 447만 원
  - 나머지 예비비/부가 리소스: ~330만 원
- **효과**
  - 저비용 Ollama 서버를 **회사 공용 실험/내부툴 LLM** 용도로 상시 운영.
  - 3개월 정도 **집중 vLLM 성능·모델 실험** 후, 온프레미스 구조와 비교/정리.

### 시나리오 B: "vLLM 중심, 고성능 벤치마크 프로젝트"

- **구성**
  - PRO6000.G1 or G2 기반 VM 2~3개월 집중 사용 (단기 고비용).
  - 남은 크레딧은 T4/G1 같은 저가 VM으로 롱테일 실험/백업용.
- **효과**
  - Qwen3 235B, GPT-OSS 120B, Llama3.3 70B 등의 **실전급 모델을 vLLM으로 마음껏 실험**.
  - 이후 온프레미스 RTX 5080 ×3에 맞는 축소판/대치 모델을 선택.

### 시나리오 C: "맞춤형 GPU 서버 1대 + 온프레미스 + Ollama"

- **구성**
  - 맞춤형 PRO6000 Multi-GPU 서버 1대 1~2개월 집중 테스트.
  - 온프레미스 RTX 5080 클러스터와 vLLM API 인터페이스를 통일.
- **효과**
  - 최종적으로 **PRO6000급 상시 서버가 꼭 필요한지** 여부를 판단.
  - 향후 온프레미스 확장(RTX 5080 추가 vs PRO6000급 장비 구매) 의사결정에도 도움.

---

## 6. 문서화 및 다음 단계

- 이 문서는 iwinv가 제공하는 **Ollama / vLLM 상품 스펙과 활용 방향을 한곳에 정리**하기 위한 메모이다.
- 실제 선택은:
  1. **우리 온프레미스 RTX 5080 ×3 클러스터의 역할을 최대한 살리는 것**(사내 KB + 임베딩/리랭킹 + RAG),
  2. iwinv 클라우드는 **실험용/확장용 GPU** 로 쓰는 방향에서 결정.
- 다음 단계 제안:
  - ① 온프레미스 gpu-03 (nixos-config) 에서 vLLM + Ollama 스택을 완성하고,
  - ② 이 문서에 기반해 iwinv 측에 간단한 요구사항(필요 GPU, 기간, 예산)을 정리,
  - ③ 실제 계약 전에 1~2개의 시나리오(A/B/C 중 하나)를 택해 PoC 플랜으로 구체화한다.
